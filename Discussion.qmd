---
bibliography: references.bib
---

# Discussion {#sec-discussion}

## Pros and Cons of Using a Non-Homogeneous Poisson Process

As outlined in the methodology (@sec-nhpp), a Non-Homogeneous Poisson Process (NHPP) is used to model the temporal clustering of extremes across four stations in NSW. A NHPP model allows for process intensity to be a time-varying function of covariates, offering improvements on a Homogeneous Poisson Process (HPP) that assumes a constant rate of extreme occurrence.

Use of a NHPP provides covariate parameter estimates that are linearly interpretable, making model findings more understandable for those without a sufficient statistical background, i.e. in a policy setting. The covariate set for each model can be extended to include additional subseasonal and seasonal factors, which is then followed by either variable selection or regularisation techniques to identify which covariates are relevant to the extreme arrival process at a station.

Intensities over various intervals are easily obtained by integrating over the subset of intensities, providing a method of assessing temporal clustering behaviour over any desired interval. The ability to integrate across any number of days and to use variable selection techniques to obtain an Australian location-relevant model are the biggest strengths of our NHPP model. Theoretically, the methods developed in this paper can be applied to any Australian weather station and may also be adapted to model clustering behaviours in wind and temperature extremes.

As the models are using a binary series of extremes as the dependent variable under a Poisson framework, the intensity function is not bounded at 1 and may imply observing multiple extremes per day. This limitation causes the models daily intensity to lose interpretability, requiring at least 14 days for intensities to level at the stations analysed.

Further issues are created through the process of declustering extremes. Following NHPP estimation, and prior to estimating interval intensities and clustering probabilities, the relabelled extremes must be returned to the sample, however the method used to do so directly influences intensities for an interval. The observations can either be returned to the sample as zero, given their declustered status, or use parameter estimates and covariate values for the day to estimate intensity, regaining its extreme-like intensity as consequence.

In an attempt to retain intepretability, parameters remain linear, however the use of a GAM-like framework [@barton2022] would potentially model the relationship between covariates and extremes more appropriately, at the cost of interpretability.

## Modelling Extremes at Different Timescales

Localised and large-scale atmospheric drivers have different timescales of predictability, with the predictive accuracy for localised atmospheric conditions declining rapidly relative to large-scale drivers. To reflect the differences in suitable timescales of predictability for these drivers, this paper uses a framework consisting of a subseasonal model that includes localised drivers and a seasonal model that includes large-scale atmospheric drivers.

The subseasonal model is appropriate for use in intervals up to 30 days in length, however validity falls as this 30-day bound is approached, given the difficulty in predicting climate drivers such as atmospheric pressure and winds for a 30-day period. Beyond 30 days, the seasonal model is required as variation in large-scale drivers such as ENSO and IOD is more predictable on a seasonal to annual scale. Similar to the subseasonal model, performance deteriorates for the seasonal model as the timescale increases, consistent with expectations. As the majority of day-to-day variation in intensity is driven by localised atmospheric drivers, the seasonal models are not appropriate on a subseasonal level, however provide a useful medium-term outlook.

To integrate across intervals of different lengths, the intensity function is assumed to be a piecewise constant, due to our datas observational frequency, on the day it is estimated (@sec-interval). Whilst an appropriate simplifying assumption for our purposes, it does not capture the reality that intraday intensity would not be constant. Under the methodology used in this paper, an extreme rainfall event that occurs due to 24 hours of constant rain is no different to an extreme that is consequence of 2 hours of heavy rain. With these two examples, it is apparent that the intraday intensity function would differ between the two, despite being treated as a constant for our purposes. The framework we have used here could be extended with sub-daily observations to model intraday intensity and similar integration techniques (@eq-int) used to more appropriately represent daily intensity at a station.

## Methods for Dealing with Data Missingness

Daily rainfall is considered at the station level, creating issues with periods of data missingness where observations do not exist. To manage this missingness, gridded data at a 2.5x2.5 degree resolution is used [@kalnay1996]. Gridded data interpolates a gridded total from station observations within the grid, leading to issues in extremes analysis as rainfall is a highly localised process and gridding smooths the localised nature of rainfall.

A simple approach to manage missingness is to remove days with missing data points entirely, although this approach would impact the daily and interval intensity functions, requiring an interpolated value to estimate these intensities. An alternative simple approach would be to forward fill observations, implying that rainfall totals yesterday are a suitable approximation for today if no data is present. Forward filling is appropriate where the duration of missingness is short. However, issues arise when there are prolonged periods of missingness, as the last observed value will be imputed for multiple days, which is highly detrimental if an extreme is the last day with an observed rainfall total.

Traditional time series approaches based on the historical data up to the time of missingness could be applied to estimate missing values, such as ARIMA models and Kalman Filters [@kohn1986]. These approaches are limited during periods of missingness as they are mean reverting and would still result in periods without extremes. Modern approaches to estimating missing data points may make use of recurrent neural networks such as the LSTM algorithm [@hochreiter1997], however this may be at a cost to computational efficiency.

As noted, this paper uses gridded data to manage the missingness issue as it provides a 'true' approximation for rainfall on the missing day, however each of the above methods have benefits and limitations, with the choice of the optimal missingness-handling method an opportunity for future research.

## Defining an Extreme: Magnitude vs Occurrence

In defining an extreme as a binary variable, no distinction is made between the magnitude of events. The binary classification is appropriate in the current setting as the research question is focused on the clustering of all extremes. Despite this, a clustered series of multiple 'small' extremes is likely to carry less harmful impact than a clustered series of 'large' extremes. This causes an information gap with the current model, as not all extremes are equal, and larger exceedances are more likely to be burdensome to the impacted communities. As consequence, a more thorough analysis of extreme clustering requires consideration of event magnitude.

A model for the exceedances could be created from the same covariate set used to model the clustering of events, providing estimates on both the expected exceedance and the relationship between exceedances and the covariates. An improved understanding of the relationship between the size of an exceedance, clustering of large exceedances and underlying climate drivers will help further inform risk management practices.

Future research directions incorporate exceedances into the clustering model, providing an estimate of the exceedances for given climate driver values. Information produced by both the temporal clustering model and the exceedance model can inform policy and planning decisions as to the hazard present before/during an event.

## The Clustering of Extremes in the Hawkesbury

The results produced by this paper indicate that there is evidence of clustered extremes in the Hawkesbury region and that the region is more susceptible to summertime extremes, the phase of ENSO and East Coast Lows (ECL) than coastal NSW. Each of these findings has implications for risk management and policy purposes.

With there being evidence of extremes being temporally clustered at both Richmond and Kurrajong Heights, the risk of flooding due to compounding extreme rainfall events is heightened. Similar clustering behaviour is not present at either Sydney or Newcastle. Given the clustering present at the Hawkesbury stations, use of a NHPP is supported, as there is evidence of the arrival process not being constant across time. This is in contrast to Sydney and Newcastle where the use of a NHPP is less supported as there is a lack of evidence the arrival process differs from a process with a constant rate of occurrence. Further, as extremes were declustered with a run length, $r$, of 2 days, the impact of consecutive extreme days is not considered in the estimated models, understating immediate-term clustering. The evidence of clustering at Hawkesbury stations indicates a need for increased compound event planning in the region as the river is more prone to flooding with sequential events.

Seasonality in the extreme arrival process is at its maximum intensity during February for the Hawkesbury stations and at its minimum intensity during August. This aligns with the monthly rainfall distribution at the Hawkesbury stations, showing that the majority of rain occurs over the summer months. Intensities peak and trough during April and October respectively for the coastal stations. Whilst the majority of rainfall occurs during the summer months, consideration should be given to the time distribution of extremes not being identical to distribution of all rainfall. As process intensity is heightened during summer for the Hawkesbury but not coastal NSW, resources could be more appropriately allocated to this region over the summer period.

ENSO is a significant driver of the seasonal to annual extreme arrival process at the Hawkesbury stations, contrasting with its lack of impact on the coastal stations. This result has implications for planning decisions during La Niña periods as the Hawkesbury region is at an enhanced flood risk, relative to Sydney and Newcastle. Currently, consideration is only given to the monthly average of the SOI and by proxy the strength of the ENSO phase. Future directions could consider the build-up effect to model the impact of ENSO periods such as the 2020-2022 triple dip La Niña on the clustering and magnitude of extremes.

The relative magnitude of wind and pressure parameters in the Hawkesbury models suggests that the region is more likely to experience an extreme than coastal NSW during an East Coast Low. As ECLs are the predominant driver of rainfall for much of eastern and southern Australia, the Hawkesbury susceptibility is an important finding due to their semi-regular occurrence. As the current model handles ENSO, wind and pressure separately, the interaction between ECLs and ENSO needs to be further explored. With evidence of clustering being present at Hawkesbury stations, the region requires increased monitoring when multiple ECLs form in a short period of time.
