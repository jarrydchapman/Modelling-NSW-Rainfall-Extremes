---
bibliography: references.bib
---

# Methodology {#sec-methods}

```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE}
library(tidyverse)
library(extRemes)
library(gridExtra)
```

```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE}
richmond <- read_csv("richmond.csv")
```

```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE}
richmond.nonzero <- richmond %>%
  filter(mm > 0)
```

The following section provides a methodological background for defining extreme rainfall events (@sec-extremeseries), analysing the relationship between climate drivers and extremes (@sec-perm), detecting the presence of temporal clustering (@sec-ripley) and Poisson process modelling for the arrival and clustering of extremes (@sec-nhpp).

## Creating an Extreme Rainfall Event Series {#sec-extremeseries}

The first step in modelling extremes is determining days on which an extreme rainfall event occurred. To do so, the Peak Over Threshold (POT) method is adopted [@coles2001], where days that have rainfall totals exceeding a threshold amount are considered extreme. The POT method is appropriate as it allows for the consideration of extremes at a daily level, as opposed to alternative extreme value theory approaches which may consider an extreme as the maximum observed rainfall over a block of time.

Extreme value theory shows that for a sufficiently large threshold, $u$, the distribution of exceedances, $(X - u)$, follows a Generalised Pareto Distribution (GPD), where $X$ is the random variable of interest. Mathematically, this is given by:

$$
Pr(X > u + y \mid X > u) = \left[1 + \frac {\xi y} {\tilde \sigma}\right]^{-1/\xi},
$$ {#eq-gpd}

where

$$
\tilde \sigma = \sigma + \xi (u - \mu).
$$

The GPD distribution is characterised by three parameters: the location parameter, $\mu$, controlling the distributions central tendency, the scale parameter, $\sigma$, controlling the distributions variability, and the shape parameter, $\xi$, which controls the heaviness and direction of the distributions tail. The shape parameter is the most important driver of distributional behaviour.

Selection of a suitable threshold involves a bias-variance trade-off and the need to retain enough data for estimation purposes. Where the threshold is too low, high bias will occur as the GPD distributional assumptions do not hold, however there will be more data points for estimation purposes. Where the threshold is too high, high variance will occur as there are not enough data points for estimation.

For threshold selection, only days on which precipitation occurred are considered. By considering only the days on which rainfall occurred, the threshold is unaffected by the count of zero rainfall days which would otherwise lower the amount of rainfall determined to be extreme. Across the stations analysed in this paper, the 95th percentile of rainfall on rain-only days is approximately equivalent to the 98th percentile of rainfall on all days and the 97.5th percentile of rainfall on rain-only days is approximately equivalent to the 99th percentile of rainfall on all days.

To select an appropriate threshold, GPDs are estimated at each location across thresholds between the 90th and 99.5th percentile of rainfall on rain-only days. The parameter estimates from the set of estimated GPDs are evaluated with the lowest threshold that provides stable scale and shape parameter estimates being chosen. This approach is valid as the theory suggests that if a threshold, $u$, follows a GPD, all higher thresholds should also follow a GPD, and parameter estimates should be approximately constant beyond a suitable threshold value [@coles2001].

```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE}
# thresholds <- threshrange.plot(richmond.nonzero$mm, 
#                  r = c(quantile(richmond.nonzero$mm, 0.9), quantile(richmond.nonzero$mm, 0.995)),
#                  type = "GP", 100, 0.1) %>%
#   as_tibble()
# 
# threshmm <- c(quantile(richmond.nonzero$mm, 0.9), rep(0, 99))
# for (i in 1:99) {
#   threshmm[i+1] <- quantile(richmond.nonzero$mm, 0.9) + (i * ((quantile(richmond.nonzero$mm, 0.995) - quantile(richmond.nonzero$mm, 0.9)) / 99))
# }
# 
# thresholds <- thresholds %>%
#   mutate(threshold = threshmm)
# 
# colours <- c("95th Percentile" = "blue",
#              "97.5th Percentile" = "green",
#              "99th Percentile" = "red")
# 
# scale <- thresholds %>%
#   ggplot() +
#   geom_point(aes(threshold, t.scale)) +
#   geom_errorbar(aes(x = threshold, ymin = low.t.scale, ymax = up.t.scale)) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.95), colour = "95th Percentile")) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.975), colour = "97.5th Percentile")) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.99), colour = "99th Percentile")) +
#   scale_colour_manual(values = colours) +
#   labs(x = "Threshold (mm)",
#        y = "Reparameterised Scale",
#        colour = "Threshold") +
#   theme_bw()
# 
# shape <- thresholds %>%
#   ggplot() +
#   geom_point(aes(threshold, shape)) +
#   geom_errorbar(aes(x = threshold, ymin = low.shape, ymax = up.shape)) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.95), colour = "95th Percentile")) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.975), colour = "97.5th Percentile")) +
#   geom_vline(aes(xintercept = quantile(richmond.nonzero$mm, 0.99), colour = "99th Percentile")) +
#   scale_colour_manual(values = colours) +
#   labs(x = "Threshold (mm)",
#        y = "Shape",
#        colour = "Threshold") +
#   theme_bw()
```

```{r, include = FALSE, message = FALSE, warning = FALSE, echo = FALSE, fig.align = 'center'}
# #| label: fig-gpdsel
# #| fig-cap: "GPD parameter estimates across thresholds at Richmond UWS Hawkesbury, showing relative parameter stability between the 97.5th and 99th percentiles, with reparameterised scale used to provide a stationary as opposed to linear interpretation"
# grid.arrange(scale, shape)
```

![Plot of GPD parameter stability across thresholds, showing relative stability between the 97.5th and 99th percentile, with reparameterised scale used to provide a stationary as opposed to linear interpretation.](images%5Cthresh.png){#fig-sel}

Threshold selection via parameter stability can be subjective, however visual inspection of parameter estimates across thresholds is an issue common across many areas of statistics, for example the use of scree plots to determine an appropriate number of principal components. Using Richmond UWS Hawkesbury as example, @fig-sel illustrates the subjective nature of locating parameter stability. This research defines an extreme event as a day on which accumulated rainfall exceeds the 97.5th percentile of rain-only day rainfall, with threshold amounts being location-specific.

Following selection of an appropriate threshold, the GPD serves no further purpose for this research. Given the focus on the temporal clustering of extremes, extreme occurrence is the quantity of interest rather than the level of exceedance. However, future research will incorporate exceedances into the approach developed by this paper.

With the threshold determined, a binary series is created to reflect which days an extreme rainfall event occurred, $Y_t$, where $t$ indexes the day, with values of 1 representing days on which an extreme event occurred and 0 otherwise. Mathematically, this is given by:

$$
Y_{t} = \left \{
\begin{aligned}
&1, && \text{if} \ X > u \\
&0, && \text{otherwise.}
\end{aligned} \right.
$$ {#eq-yt}

As the methods used to model the temporal clustering of extremes assume independent arrivals, the occurrence series must be adjusted to remove extremes that exhibit short-term dependence. To remove short-term temporal dependence, runs declustering is applied [@ferro2003; @fukutome2015]. Runs declustering removes short-term dependence by considering any extremes within $r$ days of each other to be part of the same event. The declustered extremes series can be expressed as follows:

Let $Y_i$ be the day of extreme event $i$,

then

$$
m_{i} = \text{arg}\max\limits_{t^{*}}\, [X(t^*)], \quad t^* \in [t-r, ..., t+r],
$$ {#eq-dc}

finally let the declustered series, $\tilde Y_{t}$, be defined as:

$$
\tilde Y_{t} = \left \{
\begin{aligned}
&Y(m_{i}) = 1, \\
&Y(g) = 0, && \text{for} && g \in [t-r,...,t+r] && \text{and} && t \ne m_{i}.
\end{aligned} \right.
$$ {#eq-ytdc}

A run length, $r$, of 2 is used in this study, consistent with the meteorological literature related to mid-latitude weather systems [@lackmann2011], with the day that received the maximum amount of rainfall chosen to represent the arrival of the extreme.

For the discussion that follows, all references to extreme events is with regard to the declustered series of binary extremes, unless specified otherwise.

## Randomisation Testing for the Significance of Large-Scale Drivers on Extreme Event Arrivals {#sec-perm}

Permutation testing is used as an exploratory technique at each station to indicate whether there is a relationship between the occurrence of extreme events and the three predominant large-scale climate drivers impacting Eastern Australia: ENSO, IOD and, SAM.

The data is resampled without replacement, and permutation-based testing for proportions is designed to break the relationship between the variables of interest [@collingridge2013]. By breaking the association between the variables of interest, an indication as to whether the observed difference in proportions is due to chance and therefore independent, can be obtained.

At each location the tests for each climate driver are established as follows, using ENSO to demonstrate:

1.  Two new binary variables are created indicating whether an extreme occurred on that day and whether the climate driver is in a phase likely to result in increased rainfall, i.e. whether ENSO is in a La Niña phase.

2.  The proportion of extremes occurring under each climate driver phase is calculated, $\eta_{1}$ and $\eta_{2}$, with the difference in sample proportions the statistic of interest, $d_{o}$, given by:

$$
d_{o} = \eta_{1} - \eta_{0}.
$$ {#eq-diff}

3.  The data is resampled without replacement $S$ times, with the difference in sample proportions calculated for each permuted sample, $d_{s}^{v}$, where $s$ denotes a simulated difference and $v$ represents the $v$th simulated sample.

4.  The significance of the observed difference in sample proportions is assessed by comparing the observed statistic against the estimated differences from the permuted samples ($S$ = 100), with an empirical $p$-value given by:

$$
p = \frac {\sum^{S}_{s=1} (d_{s}^{v} \geq d_{o})} {S}.
$$ {#eq-permp}

Dependence between the occurrence of extremes and climate driver phase is considered present if the empirical $p$-value is less than $\alpha = 0.01$, with a null hypothesis of independence between extreme occurrence and climate driver phase.

## Ripley's K Function for Testing the Presence of Temporal Clustering in Extremes {#sec-ripley}

With the short-term dependence in extremes removed, the focus shifts to detecting the clustering of extreme event arrivals on a subseasonal timescale.

A Homogeneous Poisson Process (HPP) can be used to model the rate of extreme occurrence at a station. A HPP assumes a constant rate of occurrence, with intensity, $\lambda$, equal to the sample mean of $Y_{t}$. HPPs provide an appropriate baseline for model evaluation and detecting the presence of temporally clustered extremes, as each point is independent, exhibiting no temporal dependence and therefore no clustering behaviour [@barton2016].

To test for the presence of temporal clustering in rainfall extremes within a 1-30 day timespan, a one-dimensional in time Ripley's K function is implemented [@ripley2005; @tuel2021]. When evaluated in terms of time, the function provides the expected number of extreme event arrivals within $j$ days, conditional on the occurrence of an extreme. For day $j$, the function is defined by:

$$
K_{j} = \lambda^{-1} \left(\mathbb{E} \left[\sum^{j}_{k=1} Y_{t+k} \mid Y_{t} = 1\right] - 1\right),
$$ {#eq-kfun}

where $\lambda$ is the average density of extremes at the location, equivalent to $\lambda$ in a HPP, and $Y_{t}$ is the binary series of extremes.

A baseline rate of occurrence is determined by simulating $S$ HPPs with a constant intensity equal to the station intensity $\lambda$, simplifying the K function (@eq-kfun) to:

$$
K_{j} = \left(\mathbb{E} \left[\sum^{j}_{k=1} Y_{t+k} \mid Y_{t} = 1\right] - 1\right).
$$ {#eq-kfunsimp}

To detect the presence of temporal clustering, the empirical distribution of the simulated HPPs ($S$ = 100) and the K function value at each day $j$ are compared, with an empirical $p$-value, $p_{j}^{k}$, given by:

$$
p_{j}^{k} = 1 - \left(\frac {\sum^{S}_{s=1} (K_{n} > HPP_{n,s})} {S}\right).
$$ {#eq-prip}

The occurrence of an extreme event is considered temporally clustered within $j$ days of an extreme if the empirical $p$-value is less than $\alpha = 0.01$.

## A Non-Homogeneous Poisson Process to Model Extreme Rainfall Event Arrivals {#sec-nhpp}

To model the extreme arrivals process and clustering behaviour, Poisson processes are used. The arrivals of extreme events occurring over time can be viewed as a counting process, starting at zero, with the number of arrivals at time $t$ given by ${N(t), t \geq 0}$. Counts of data are frequently represented as coming from a Poisson distribution, which exhibits equi-dispersion and is completely specified by a single parameter, $\lambda$. Under the Poisson distribution, the probability mass function (PMF) is as follows:

$$
f(x) = \frac {e^{-\lambda} \lambda^x} {x!},
$$ {#eq-poipmf}

where $x$ is a non-negative integer and $\lambda$ is a positive number capturing the mean and variance of the distribution.

Poisson processes are typically used to model counts of positive integers, however they can model a binary series provided $n$ is large and $p$ is small, where $n$ is the sample size and $p$ is the probability of occurrence. Extreme events by definition rarely occur, resulting in a low $p$, and large $n$. The Poisson-Binomial approximation is given by:

$$
{n \choose x} p^{x} (1-p)^{n-x} = \frac {n!} {x!(n-x)!} \left(\frac {\lambda} {n}\right)^{x} \left(1 - \frac {\lambda} {n}\right)^{n-x},
$$

$$
\approx \frac {\lambda^{x}} {x!} \left(1 - \frac {\lambda} {n}\right)^{n},
$$

$$
\approx \frac {e^{-\lambda}\lambda^{x}} {x!}. 
$$ {#eq-poibin}

Previous contributions to the climate extremes literature have characterised the series of extreme occurrences as a homogeneous point process [@villarini2011], which relies on the arrival process of extremes converging to a homogeneous Poisson process in its limit [@chen1975]. For a series to exhibit Poisson homogeneity, it must have constant and equivalent mean and variance across time. Whilst a restrictive assumption, homogeneous Poisson processes provide a useful baseline for model evaluation, as they fit a constant intensity to which more sophisticated techniques can be compared to. A Non-Homogeneous Poisson Process (NHPP) can be used to loosen the HPP assumptions, allowing the intensity of the process to vary in time and as a function of time-varying covariates.

A NHPP can characterise the daily extreme arrival process as a counting process [@streit2010; @keeler2016], with the expected number of events up to time $t$ given by:

$$
\Lambda(t) = E[N(t)] = \int^{t}_{0} \lambda(t) dt.
$$ {#eq-expectationt}

In a NHPP, $\lambda(t)$ models the time-varying intensity of the process on a given day and $\Lambda(t)$ represents the expected number of events up to time $t$.

The PMF and $\Lambda(t)$ can be used to calculate the probability of observing a given number of events, $k$, occurring up to time $t$, given by:

$$
Pr[N(t) = k] = \frac {e^{-\Lambda(t)} \Lambda(t)^k} {k!}.
$$ {#eq-probt}

The NHPP for the daily arrival process allows for the intensity to vary with time as a function of a set of standardised covariates, with the fitted intensity at time $t$ given by:

$$
\hat \lambda(t) = e^{X_t' \hat \beta},
$$ {#eq-regfun}

where $X_{t}$ is a vector of covariates at time $t$ and $\hat\beta$ is a vector of estimated coefficients for each covariate.

The declustering process relabels several extremes, these extremes will have covariate values reflective of an extreme event despite being considered a non-extreme post declustering. NHPP parameter estimation must be performed with these days removed from the sample, as inclusion of these days will bias parameter estimates downwards given their classification as a non-extreme.

The parameters of the NHPP are then estimated via maximum likelihood estimation on the adjusted sample. The log-likehood function for NHPP parameter estimation is:

$$
\ell(\beta) = \sum_{t=1}^{T}[-\lambda(t) + Y_{t}\log[\lambda(t)] - \log (Y_{t}!)],
$$ {#eq-mle}

given extremes are binary, @eq-mle reduces to:

$$
\ell(\beta) = -\sum^{T}_{t=1} \lambda(t) + \sum^{T}_{i=1} \log[\lambda(t)],
$$ {#eq-mlered}

where $T$ is the number of days in the sample.

### The Poisson Model Set

To model the extreme arrival process and the impact of relevant atmospheric drivers on extreme occurrence, several Poisson processes are fit to each station. The models are defined as follows:

A Homogeneous Poisson Process is used to capture the baseline intensity of the extreme arrival process. The intensity is constant across the sample and is equal to the average occurrence rate in each period. The fitted HPP at each station can be used for comparison with the set of NHPPs and for model validation. Model 1 is given by:

$$
M_{1}: \lambda = \exp(\beta_{0}) = \frac {1} {T} \sum^{T}_{t=1} Y_{t}.
$$ {#eq-m1}

Model 2 is designed to incorporate the seasonality inherent in the occurrence of extreme rainfall events (@fig-seasonal). The underlying seasonality is modelled as a function of harmonic terms representing each day of the year. As the harmonics are the only covariates in this model, the fitted intensity of the process is constant year on year. Model 2 is given by:

$$
M_{2}: \lambda(t) = \exp\left[\beta_{0} + \beta_{1}\cos\left(\frac {2 \pi t} {m}\right) + \beta_{2} \sin\left(\frac {2 \pi t} {m}\right) + e_{t}\right].
$$ {#eq-m2}

Weather and localised atmospheric drivers are the predominant subseasonal causes of extreme rainfall events. To model the subseasonal extreme arrivals process, a model including covariates for the harmonics, cumulative 30-days of lagged rainfall, $\zeta$, mean sea level pressure, $\kappa$, and U-winds at 250hpa, $\gamma$, and 850hpa, $\omega$, is adopted. The fitted intensity is time-varying and is suitable for a 14-30 day timescale of prediction as the included covariates are only predictable on a subseasonal to seasonal (S2S) scale. The combination of pressure and wind covariates is designed to model the impact of East Coast Lows on extreme event occurrence. Model 3 is given by:

$$
M_{3}: \lambda(t) = \exp\left[\beta_{0} + \beta_{1}\cos\left(\frac {2 \pi t} {m}\right)
 + \beta_{2}\sin \left(\frac {2 \pi t} {m}\right) + \beta_{3}\zeta_{t} + \beta_{4}\kappa_{t} + \beta_{5}\gamma_{t} + \beta_{6}\omega_{t} + e_{t}\right].
$$ {#eq-m3}

Large-scale atmospheric drivers impact the occurrence of extreme rainfall events on a seasonal scale. The covariate set for a model reflecting a seasonal scale includes harmonic terms, the SOI index, $\psi$, as proxy for ENSO and the DMI index, $\theta$, as proxy for IOD. This model is more appropriate on a seasonal to annual (S2A) timescale, as SST anomalies such as ENSO have a greater degree of S2A predictability than short-term atmospheric drivers. Other large-scale climate drivers impacting the eastern seaboard could be included in this model, however permutation testing indicates that ENSO is the only climate driver that is of significance in this region (@fig-perm). Model 4 is given by:

$$
M_{4}: \lambda(t) = \exp \left[\beta_{0} + \beta_{1} \cos \left(\frac {2 \pi t} {m} \right) + \beta_{2} \sin \left(\frac {2 \pi t} {m} \right) + \beta_{3}\psi_{t} + \beta_{4}\theta_{t} +  e_{t} \right].
$$ {#eq-m4}

At the daily level, both subseasonal and seasonal climate drivers influence the occurrence of extremes despite having different applicable timescales. The full model incorporates all covariates considered in both the subseasonal and seasonal models. This model does not align with the timescales of predictability as it incorporates variables of differing predictability, however the models insights remain valuable. Model 5 is given by:

$$
\begin{aligned}
M_{5}: \lambda(t) = \exp\biggl[ & \beta_{0} + \beta_{1}\cos\left(\frac {2 \pi t} {m}\right)
+ \beta_{2}\sin \left(\frac {2 \pi t} {m}\right) \\
&+ \beta_{3}\zeta_{t} + \beta_{4}\kappa_{t} + \beta_{5}\gamma_{t}
+ \beta_{6}\omega_{t} + \beta_{7}\psi_{t} + \beta_{8}\theta_{t} + e_{t} \biggr].
\end{aligned}
$$ {#eq-m5}

Models 3-5 use stepwise variable selection algorithm with AIC as the information criterion [@akaike1974]. The algorithm is initialised with the full covariate set, as defined for each model above, and then calculates AIC at each step for either removing or adding covariates to the model. The algorithm can produce non-optimal solutions, however is a suitable proxy for 'best' as it is designed to provide a covariate set that minimises AIC. The stepwise selection process may produce different covariate sets at each location, providing an indication as to which factors drive the extreme rainfall arrival process at the station.

### Model Validation for Non-Homogeneous Poisson Processes {#sec-nhppvalmeth}

Model validation is performed by analysing the raw residuals against time [@baddeley2005] and by transforming the NHPP into a HPP to assess autocorrelation with a Ljung-Box test and to evaluate model behaviour with a Kolmogorov-Smirnov test [@kim2015].

A NHPP is transformed into a HPP using a time scale transformation [@daley2003], where the occurrence points in the HPP are given by:

$$
t^{H}_{i} = \int^{t^{NH}_{i}}_{0} \lambda(t)dt,
$$ {#eq-transform}

where $t^{H}_{i}$ and $t^{NH}_{i}$ refer to occurrence points in the transformed HPP and NHPP respectively.

The exponential residuals of the HPP, equal to the distance between events, are used to create a uniform distribution, with the uniform residuals given by $\exp(-d_{i})$ [@ogata1988]. Provided the exponential residuals follow a uniform distribution and there is no serial correlation in the exponential residuals, a NHPP is a suitable method for modelling extreme arrivals.

`R` is used for model estimation and validation. Model validation is performed using the `NHPoisson` package in R [@cebrián2015].

## Extending the Non-Homogeneous Poisson Process to Disjoint Intervals of Time {#sec-interval}

As each model is fit at a daily level, and the research focus is on the temporal clustering of events, methods are required to transition from daily intensities to intensities over disjoint intervals of time. The process for calculating the expected number of events, $\Lambda(t)$, can be extended to considering same length, disjoint intervals of time, $B$, of length $l$, and determining the expected number of events in that interval.

To estimate interval intensities, it is assumed that each disjoint interval is independent and that fitted daily intensities are piecewise constants. With covariates and extreme occurrences only being observed daily, a piecewise constant assumption is appropriate, despite the fitted intensity function being continuous.

The expected number of events in an interval is given by:

$$
\Lambda(B_{i}) = E[N(b) - N(a)] = \int^{b}_{a} \lambda(t) dt.
$$ {#eq-int}

With observations taken at a daily level, @eq-int can be expressed as:

$$
\Lambda(B_{i}) = \int_{a}^{a+1} \lambda_{a+1}dt + \int_{a+1}^{a+2} \lambda_{a+2}dt + \ldots + \int_{a+n-1}^{b} \lambda_{b}dt,
$$ {#eq-expand}

which reduces @eq-expand to:

$$
\Lambda(B_{i}) = \lambda_{a+1} + \lambda_{a+2} + \ldots + \lambda_{b} = \sum_{t=a}^{t=b} \lambda(t).
$$ {#eq-simplify}

### The Mean Under-Prediction Error to Evaluate NHPP Interval Intensities

As the potential impact of missing an extreme event outweighs risks associated with over-predicting extremes, a mean under-prediction error (MUPE) is used to quantify the average degree of under-prediction for estimated interval intensities relative to observed interval extreme arrivals, $H(B_{i})$. The MUPE is derived as follows:

Let $a_{i}$ be given by:

$$
a_{i} = \left \{
\begin{aligned}
& H(B_{i}) - \Lambda(B_{i}), && \text{if} \ H(B_{i}) > \Lambda(B_{i}) \\
&0, && \text{otherwise}
\end{aligned}, \quad \forall i, \right.
$$ {#eq-seta}

then let $A$ define the set given by:

$$
A = |\{a_{i} \mid a_{i} > 0\}|, \quad \forall i,
$$ {#eq-setc}

finally the MUPE is given by:

$$
MUPE = \frac {1} {\mid A \mid} \sum_{i=1}^{\mid A \mid} a_{i}.
$$ {#eq-mupe}

Further assessment of interval intensity fit is performed using traditional error metrics.

## Using Disjoint Interval Intensity to Estimate the Probability of Temporally Clustered Extremes {#sec-nhppprob}

Estimated interval intensities provide an insight into historical extremes processes and how well the NHPP is fitting the actual temporal extremes process. The intensities do not directly provide probabilistic information as to the clustering of events, however probability estimates can be easily obtained using the PMF of the Poisson distribution.

The expected number of events in an interval, of length $l$, $\Lambda(B_{i})$, can be used to estimate the probability of $k$ extremes occurring within an interval, given by:

$$
Pr[N(B_{i}) = k] = \frac {e^{-\Lambda(B_{i})} \Lambda(B_{i})^k} {k!}.
$$ {#eq-intprob}

To evaluate NHPP ability to classify extremes as temporally clustered, ROC curve analysis [@hanley1982] and classification metrics are utilised.

Bootstrap confidence intervals are constructed by resampling the data 100 times with replacement, estimating the NHPP model and fitting daily then disjoint interval intensities for each model. The relevant quantiles from each intervals empirical bootstrap distribution are used to provide 95% confidence intervals for the fitted number of events and the probability of multiple events occurring in each interval.
